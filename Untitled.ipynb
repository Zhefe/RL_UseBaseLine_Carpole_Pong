{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fd9f4e50",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install gym[atari]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8cae6ede",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import deque\n",
    "import cv2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "37581b9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper fc for pong\n",
    "def preprocess(image):\n",
    "    \"\"\" Prepro 210x160x3 uint8 frame into 6400 (80x80) 2D float array \"\"\"\n",
    "    image = image[35:195]  # crop\n",
    "    image = image[::2, ::2, 0]  # downsample by factor of 2\n",
    "    image[image == 144] = 0  # erase background (background type 1)\n",
    "    image[image == 109] = 0  # erase background (background type 2)\n",
    "    image[image != 0] = 1  # everything else (paddles, ball) set to 1\n",
    "    return np.reshape(image.astype(np.float32), [80, 80])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3224874e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# simple model of full connect\n",
    "class PolicyNetwork(nn.Module):\n",
    "    def __init__(self, input_size, output_size):\n",
    "        super(PolicyNetwork, self).__init__()\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(input_size, 200),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(200, output_size),\n",
    "            nn.Softmax(dim=-1)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, x.size(-1))\n",
    "        return self.fc(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "17272f44",
   "metadata": {},
   "outputs": [],
   "source": [
    "#public methods\n",
    "def select_action(state):\n",
    "    cu_state = torch.FloatTensor(state).unsqueeze(0)   \n",
    "    probs = policy_net(cu_state)\n",
    "    action = torch.multinomial(probs, 1).item()\n",
    "    return action\n",
    "\n",
    "############\n",
    "# apply baseline to rewards\n",
    "############\n",
    "def compute_discounted_rewards(rewards, gamma, baseline=None):\n",
    "    discounted_rewards = []\n",
    "    R = 0\n",
    "    for r in reversed(rewards):\n",
    "        R = r + gamma * R\n",
    "        discounted_rewards.insert(0, R)\n",
    "    discounted_rewards = np.array(discounted_rewards)\n",
    "    \n",
    "    # Apply baseline, reduce variance\n",
    "    if baseline is not None:\n",
    "        discounted_rewards -= baseline\n",
    "\n",
    "    return torch.FloatTensor(discounted_rewards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b87a9959",
   "metadata": {},
   "outputs": [],
   "source": [
    "# here is the training part code, we provide choose of cartpole or pong, gamma, usebaseline and epoch\n",
    "\n",
    "def train_policy_network(run='cartpole', num_episodes=1000, gamma=0.95, use_baseline=False, baseline_value=0.0):\n",
    "    all_rewards = []\n",
    "    stacked_frames = deque(maxlen=4)\n",
    "\n",
    "    for episode in range(num_episodes):\n",
    "        state = env.reset()[0]\n",
    "        if run == 'pong':\n",
    "            state = preprocess(state)\n",
    "            stacked_frames = deque([state] * 4, maxlen=4)\n",
    "            state = np.stack(stacked_frames, axis=0).flatten()\n",
    "        else:\n",
    "            state = torch.FloatTensor(state).numpy()\n",
    "\n",
    "        rewards = []\n",
    "        log_probs = []\n",
    "        \n",
    "        ############\n",
    "        # interact with env\n",
    "        ############\n",
    "        while True:\n",
    "            action = select_action(state)\n",
    "            if run == 'pong':\n",
    "                mapped_action = action + 2  # Map 0 -> 2 (RIGHT), 1 -> 3 (LEFT)\n",
    "                next_state, reward, done, _, _ = env.step(mapped_action)\n",
    "                next_state = preprocess(next_state)\n",
    "                stacked_frames.append(next_state)\n",
    "                next_state = np.stack(stacked_frames, axis=0).flatten()\n",
    "            else:\n",
    "                next_state, reward, done, _, _ = env.step(action)\n",
    "                next_state = torch.FloatTensor(next_state).numpy()\n",
    "\n",
    "            rewards.append(reward)\n",
    "\n",
    "            state_tensor = torch.FloatTensor(state)\n",
    "            action_prob = policy_net(state_tensor.unsqueeze(0))[0, action]\n",
    "            log_prob = torch.log(action_prob)\n",
    "            log_probs.append(log_prob)\n",
    "            \n",
    "            state = next_state\n",
    "\n",
    "            if done:\n",
    "                break\n",
    "\n",
    "        ############\n",
    "        # optimize the model\n",
    "        ############\n",
    "        episode_reward = sum(rewards)\n",
    "        all_rewards.append(episode_reward)\n",
    "\n",
    "        baseline = baseline_value if use_baseline else np.mean(all_rewards[-100:]) if len(all_rewards) >= 100 else None\n",
    "\n",
    "        discounted_rewards = compute_discounted_rewards(rewards, gamma, baseline)\n",
    "        discounted_rewards = (discounted_rewards - discounted_rewards.mean()) / (discounted_rewards.std() + 1e-8)\n",
    "\n",
    "        loss = -torch.sum(torch.stack(log_probs) * discounted_rewards)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if episode % 100 == 0:\n",
    "            print(f'Episode {episode}, Total Reward: {episode_reward}')\n",
    "\n",
    "    return all_rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9b270bfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# here is the eval part code, we provide choose of cartpole or pong\n",
    "\n",
    "def evaluate_policy(run='cartpole', num_episodes=500):\n",
    "    rewards = []\n",
    "    stacked_frames = deque(maxlen=4)\n",
    "    \n",
    "    for episode in range(num_episodes):\n",
    "        state = env.reset()[0]\n",
    "        if run == 'pong':\n",
    "            state = preprocess(state)\n",
    "            stacked_frames = deque([state] * 4, maxlen=4)\n",
    "            state = np.stack(stacked_frames, axis=0).flatten()\n",
    "        else:\n",
    "            state = torch.FloatTensor(state).numpy()\n",
    "\n",
    "        total_reward = 0\n",
    "\n",
    "        while True:\n",
    "            action = select_action(state)\n",
    "            if run == 'pong':\n",
    "                mapped_action = action + 2\n",
    "                next_state, reward, done, _, _ = env.step(mapped_action)\n",
    "                next_state = preprocess(next_state)\n",
    "                stacked_frames.append(next_state)\n",
    "                next_state = np.stack(stacked_frames, axis=0).flatten()\n",
    "            else:\n",
    "                next_state, reward, done, _, _ = env.step(action)\n",
    "                next_state = torch.FloatTensor(next_state).numpy()\n",
    "\n",
    "            total_reward += reward\n",
    "            state = next_state\n",
    "\n",
    "            if done:\n",
    "                break\n",
    "\n",
    "        rewards.append(total_reward)\n",
    "    \n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.hist(rewards, bins=20, edgecolor='black', alpha=0.7)\n",
    "    plt.xlabel('Total Reward')\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.title('Histogram of Total Rewards Over 500 Episodes')\n",
    "    plt.show()\n",
    "    \n",
    "    mean_reward = np.mean(rewards)\n",
    "    std_reward = np.std(rewards)\n",
    "    print(f'Mean Reward: {mean_reward}')\n",
    "    print(f'Standard Deviation of Reward: {std_reward}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4db3d0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 0, Total Reward: 12.0\n",
      "Episode 100, Total Reward: 26.0\n",
      "Episode 200, Total Reward: 79.0\n",
      "Episode 300, Total Reward: 131.0\n"
     ]
    }
   ],
   "source": [
    "run = 'carpole'\n",
    "if_use_bl = False\n",
    "\n",
    "\n",
    "if run == 'pong':\n",
    "    gym.make(\"ALE/Pong-v5\")\n",
    "    input_size = 80 * 80 * 4 \n",
    "    output_size = 2  # We only use left or right\n",
    "    the_gamma = 0.99\n",
    "else:\n",
    "    env = gym.make('CartPole-v1')\n",
    "    input_size = env.observation_space.shape[0]\n",
    "    output_size = env.action_space.n\n",
    "    the_gamma = 0.95\n",
    "\n",
    "policy_net = PolicyNetwork(input_size=input_size, output_size=output_size)\n",
    "optimizer = optim.Adam(policy_net.parameters(), lr=0.01)\n",
    "\n",
    "# Train network\n",
    "all_rewards = train_policy_network(run=run, num_episodes=1000, gamma=the_gamma, use_baseline=if_use_bl, baseline_value=0.0)\n",
    "print(\"train down\")\n",
    "\n",
    "# Plot rewards\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(all_rewards, label='Episode Reward')\n",
    "moving_avg = np.convolve(all_rewards, np.ones(100)/100, mode='valid')\n",
    "plt.plot(range(99, len(all_rewards)), moving_avg, label='100-Episode Moving Average', color='red')\n",
    "plt.xlabel('Episodes')\n",
    "plt.ylabel('Total Reward')\n",
    "plt.title('Episode Reward vs Number of Episodes')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Evaluate the trained policy\n",
    "evaluate_policy(run=run, num_episodes=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1fe0167",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
